{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf49f201-8d53-4d43-9b8c-34455c59a74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define paths for the processed datasets\n",
    "processed_dir = r\"C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Data-Pipeline-for-Global-Superstore-Analytics\\Dataset\\processed_backup\"\n",
    "sales_data_path = os.path.join(processed_dir, \"superstore_sales_backup.csv\")\n",
    "geolocation_data_path = os.path.join(processed_dir, \"geolocation_backup.csv\")\n",
    "\n",
    "# Load the Superstore Sales and Geolocation datasets\n",
    "print(\"Loading processed data...\")\n",
    "sales_data = pd.read_csv(sales_data_path)\n",
    "geolocation_data = pd.read_csv(geolocation_data_path)\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591b3b4b-caea-4a8f-896f-9c643e5b1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sales data statistics:\n",
      "             Row ID        Order ID  Order Date   Ship Date       Ship Mode  \\\n",
      "count   9800.000000            9800        9800        9800            9800   \n",
      "unique          NaN            4922        1230        1326               4   \n",
      "top             NaN  CA-2018-100111  05/09/2017  26/09/2018  Standard Class   \n",
      "freq            NaN              14          38          34            5859   \n",
      "mean    4900.500000             NaN         NaN         NaN             NaN   \n",
      "std     2829.160653             NaN         NaN         NaN             NaN   \n",
      "min        1.000000             NaN         NaN         NaN             NaN   \n",
      "25%     2450.750000             NaN         NaN         NaN             NaN   \n",
      "50%     4900.500000             NaN         NaN         NaN             NaN   \n",
      "75%     7350.250000             NaN         NaN         NaN             NaN   \n",
      "max     9800.000000             NaN         NaN         NaN             NaN   \n",
      "\n",
      "       Customer ID  Customer Name   Segment        Country           City  \\\n",
      "count         9800           9800      9800           9800           9800   \n",
      "unique         793            793         3              1            529   \n",
      "top       WB-21850  William Brown  Consumer  United States  New York City   \n",
      "freq            35             35      5101           9800            891   \n",
      "mean           NaN            NaN       NaN            NaN            NaN   \n",
      "std            NaN            NaN       NaN            NaN            NaN   \n",
      "min            NaN            NaN       NaN            NaN            NaN   \n",
      "25%            NaN            NaN       NaN            NaN            NaN   \n",
      "50%            NaN            NaN       NaN            NaN            NaN   \n",
      "75%            NaN            NaN       NaN            NaN            NaN   \n",
      "max            NaN            NaN       NaN            NaN            NaN   \n",
      "\n",
      "             State   Postal Code Region       Product ID         Category  \\\n",
      "count         9800   9789.000000   9800             9800             9800   \n",
      "unique          49           NaN      4             1861                3   \n",
      "top     California           NaN   West  OFF-PA-10001970  Office Supplies   \n",
      "freq          1946           NaN   3140               19             5909   \n",
      "mean           NaN  55273.322403    NaN              NaN              NaN   \n",
      "std            NaN  32041.223413    NaN              NaN              NaN   \n",
      "min            NaN   1040.000000    NaN              NaN              NaN   \n",
      "25%            NaN  23223.000000    NaN              NaN              NaN   \n",
      "50%            NaN  58103.000000    NaN              NaN              NaN   \n",
      "75%            NaN  90008.000000    NaN              NaN              NaN   \n",
      "max            NaN  99301.000000    NaN              NaN              NaN   \n",
      "\n",
      "       Sub-Category     Product Name         Sales  \n",
      "count          9800             9800   9800.000000  \n",
      "unique           17             1849           NaN  \n",
      "top         Binders  Staple envelope           NaN  \n",
      "freq           1492               47           NaN  \n",
      "mean            NaN              NaN    230.769059  \n",
      "std             NaN              NaN    626.651875  \n",
      "min             NaN              NaN      0.444000  \n",
      "25%             NaN              NaN     17.248000  \n",
      "50%             NaN              NaN     54.490000  \n",
      "75%             NaN              NaN    210.605000  \n",
      "max             NaN              NaN  22638.480000  \n",
      "\n",
      "Geolocation data statistics:\n",
      "       iso-3166-1      country         city\n",
      "count        3331  3331.000000  3331.000000\n",
      "unique       3237          NaN          NaN\n",
      "top       Central          NaN          NaN\n",
      "freq            7          NaN          NaN\n",
      "mean          NaN    23.463472    23.574409\n",
      "std           NaN    23.071463    61.731660\n",
      "min           NaN   -54.805400  -176.543303\n",
      "25%           NaN     8.440269    -1.246162\n",
      "50%           NaN    25.204849    23.134136\n",
      "75%           NaN    41.976021    48.065996\n",
      "max           NaN    75.108613   179.017933\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display initial statistics and data preview\n",
    "print(\"\\nSales data statistics:\")\n",
    "print(sales_data.describe(include=\"all\"))\n",
    "print(\"\\nGeolocation data statistics:\")\n",
    "print(geolocation_data.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04e5a38-b045-4ca2-b02e-431a3544eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling missing values...\n",
      "\n",
      "Missing values after cleaning:\n",
      "Sales data missing values:\n",
      "Row ID            0\n",
      "Order ID          0\n",
      "Order Date        0\n",
      "Ship Date         0\n",
      "Ship Mode         0\n",
      "Customer ID       0\n",
      "Customer Name     0\n",
      "Segment           0\n",
      "Country           0\n",
      "City              0\n",
      "State             0\n",
      "Postal Code      11\n",
      "Region            0\n",
      "Product ID        0\n",
      "Category          0\n",
      "Sub-Category      0\n",
      "Product Name      0\n",
      "Sales             0\n",
      "dtype: int64\n",
      "\n",
      "Geolocation data missing values:\n",
      "iso-3166-1    0\n",
      "country       0\n",
      "city          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning: Handle missing values\n",
    "print(\"\\nHandling missing values...\")\n",
    "\n",
    "# Fill missing values in sales data\n",
    "columns_to_fill = {\"Profit\": 0, \"Discount\": 0}  # Initialize columns to handle\n",
    "if \"Quantity\" in sales_data.columns:  # Check if \"Quantity\" column exists\n",
    "    columns_to_fill[\"Quantity\"] = sales_data[\"Quantity\"].mean()\n",
    "\n",
    "sales_data.fillna(columns_to_fill, inplace=True)\n",
    "\n",
    "# Drop rows with missing essential geolocation data\n",
    "required_geolocation_columns = [\"country\", \"city\", \"Region\", \"State\"]\n",
    "available_columns = [col for col in required_geolocation_columns if col in geolocation_data.columns]\n",
    "\n",
    "if available_columns:  # Ensure columns are present before dropping\n",
    "    geolocation_data.dropna(subset=available_columns, inplace=True)\n",
    "\n",
    "# Validate missing values are resolved\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(\"Sales data missing values:\")\n",
    "print(sales_data.isnull().sum())\n",
    "print(\"\\nGeolocation data missing values:\")\n",
    "print(geolocation_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5778d47f-c7ec-4341-ae31-a45564ae4035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding new calculated columns...\n",
      "Warning: 'Profit' or 'Sales' column is missing. Cannot calculate Profit_Margin_Percent.\n",
      "\n",
      "Transformed sales data preview:\n",
      "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
      "0       1  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
      "1       2  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
      "2       3  CA-2017-138688  12/06/2017  16/06/2017    Second Class    DV-13045   \n",
      "3       4  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
      "4       5  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
      "\n",
      "     Customer Name    Segment        Country             City       State  \\\n",
      "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
      "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
      "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
      "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
      "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
      "\n",
      "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
      "0      42420.0  South  FUR-BO-10001798        Furniture    Bookcases   \n",
      "1      42420.0  South  FUR-CH-10000454        Furniture       Chairs   \n",
      "2      90036.0   West  OFF-LA-10000240  Office Supplies       Labels   \n",
      "3      33311.0  South  FUR-TA-10000577        Furniture       Tables   \n",
      "4      33311.0  South  OFF-ST-10000760  Office Supplies      Storage   \n",
      "\n",
      "                                        Product Name     Sales Sales_Category  \n",
      "0                  Bush Somerset Collection Bookcase  261.9600            Low  \n",
      "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400         Medium  \n",
      "2  Self-Adhesive Address Labels for Typewriters b...   14.6200            Low  \n",
      "3      Bretford CR4500 Series Slim Rectangular Table  957.5775         Medium  \n",
      "4                     Eldon Fold 'N Roll Cart System   22.3680            Low  \n"
     ]
    }
   ],
   "source": [
    "# Data Transformation: Create new calculated columns\n",
    "print(\"\\nAdding new calculated columns...\")\n",
    "\n",
    "# Check if the required columns exist in sales_data before creating calculated columns\n",
    "if \"Profit\" in sales_data.columns and \"Sales\" in sales_data.columns:\n",
    "    # Calculate profit margin percentage, capped between 0 and 100\n",
    "    sales_data[\"Profit_Margin_Percent\"] = (\n",
    "        (sales_data[\"Profit\"] / sales_data[\"Sales\"] * 100)\n",
    "        .replace([np.inf, -np.inf], 0)  # Handle divide-by-zero issues\n",
    "        .clip(0, 100)  # Ensure values are within 0-100 range\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: 'Profit' or 'Sales' column is missing. Cannot calculate Profit_Margin_Percent.\")\n",
    "\n",
    "# Sales category classification based on thresholds\n",
    "if \"Sales\" in sales_data.columns:\n",
    "    sales_data[\"Sales_Category\"] = sales_data[\"Sales\"].apply(\n",
    "        lambda x: \"High\" if x > 1000 else (\"Medium\" if x > 500 else \"Low\")\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: 'Sales' column is missing. Cannot calculate Sales_Category.\")\n",
    "\n",
    "# Display a preview of the transformed dataset\n",
    "print(\"\\nTransformed sales data preview:\")\n",
    "print(sales_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9651db0-935a-4f4d-8463-417d9914dd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['iso-3166-1', 'country', 'city'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geolocation_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b62123df-1a87-41ae-ad2f-7364430153d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode',\n",
       "       'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State',\n",
       "       'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category',\n",
       "       'Product Name', 'Sales', 'Sales_Category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b93dff15-b95a-4bc9-ae80-3b71ba2e29c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Data Columns: Index(['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode',\n",
      "       'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State',\n",
      "       'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category',\n",
      "       'Product Name', 'Sales', 'Sales_Category'],\n",
      "      dtype='object')\n",
      "Geolocation Data Columns: Index(['iso-3166-1', 'Country', 'City'], dtype='object')\n",
      "\n",
      "Merged dataset contains 9800 rows and 20 columns.\n",
      "\n",
      "Missing values in merged dataset (post-merge):\n",
      "Row ID               0\n",
      "Order ID             0\n",
      "Order Date           0\n",
      "Ship Date            0\n",
      "Ship Mode            0\n",
      "Customer ID          0\n",
      "Customer Name        0\n",
      "Segment              0\n",
      "Country              0\n",
      "City                 0\n",
      "State                0\n",
      "Postal Code         11\n",
      "Region               0\n",
      "Product ID           0\n",
      "Category             0\n",
      "Sub-Category         0\n",
      "Product Name         0\n",
      "Sales                0\n",
      "Sales_Category       0\n",
      "iso-3166-1        9800\n",
      "dtype: int64\n",
      "\n",
      "Missing values after handling post-merge:\n",
      "Row ID               0\n",
      "Order ID             0\n",
      "Order Date           0\n",
      "Ship Date            0\n",
      "Ship Mode            0\n",
      "Customer ID          0\n",
      "Customer Name        0\n",
      "Segment              0\n",
      "Country              0\n",
      "City                 0\n",
      "State                0\n",
      "Postal Code         11\n",
      "Region               0\n",
      "Product ID           0\n",
      "Category             0\n",
      "Sub-Category         0\n",
      "Product Name         0\n",
      "Sales                0\n",
      "Sales_Category       0\n",
      "iso-3166-1        9800\n",
      "dtype: int64\n",
      "\n",
      "Data transformation and enrichment completed. Transformed data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check column names to ensure they are as expected\n",
    "print(\"Sales Data Columns:\", sales_data.columns)\n",
    "print(\"Geolocation Data Columns:\", geolocation_data.columns)\n",
    "\n",
    "# Strip any whitespace in column names to avoid issues\n",
    "sales_data.columns = sales_data.columns.str.strip()\n",
    "geolocation_data.columns = geolocation_data.columns.str.strip()\n",
    "\n",
    "# Rename 'country' to 'Country' and 'city' to 'City' in geolocation_data\n",
    "geolocation_data.rename(columns={'country': 'Country', 'city': 'City'}, inplace=True)\n",
    "\n",
    "# Convert 'Country' and 'City' columns to string in both dataframes\n",
    "sales_data['Country'] = sales_data['Country'].astype(str)\n",
    "geolocation_data['Country'] = geolocation_data['Country'].astype(str)\n",
    "sales_data['City'] = sales_data['City'].astype(str)\n",
    "geolocation_data['City'] = geolocation_data['City'].astype(str)\n",
    "\n",
    "# Perform the merge\n",
    "merged_data = pd.merge(\n",
    "    sales_data,\n",
    "    geolocation_data,\n",
    "    how=\"left\",  # Left join to retain all sales data\n",
    "    on=[\"Country\", \"City\"]  # Now they should match\n",
    ")\n",
    "\n",
    "# Post-merge validation\n",
    "print(f\"\\nMerged dataset contains {merged_data.shape[0]} rows and {merged_data.shape[1]} columns.\")\n",
    "\n",
    "# Check for missing values in the merged dataset\n",
    "missing_after_merge = merged_data.isnull().sum()\n",
    "print(\"\\nMissing values in merged dataset (post-merge):\")\n",
    "print(missing_after_merge)\n",
    "\n",
    "\n",
    "# Handle missing values post-merge\n",
    "# Replace missing geolocation data with \"Unknown\" or other placeholders\n",
    "columns_to_fill = {\"Country\": \"Unknown\", \"City\": \"Unknown\", \"Latitude\": 0.0, \"Longitude\": 0.0}\n",
    "for column, fill_value in columns_to_fill.items():\n",
    "    if column in merged_data.columns:\n",
    "        merged_data[column] = merged_data[column].fillna(fill_value)  # Assign result back to column\n",
    "\n",
    "\n",
    "# Validate missing values are resolved\n",
    "print(\"\\nMissing values after handling post-merge:\")\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "# Save the enriched data to the transformed folder\n",
    "transformed_dir = r\"C:\\Users\\ADMIN\\Desktop\\Projects\\ETL-Data-Pipeline-for-Global-Superstore-Analytics\\Dataset\\transformed\"\n",
    "os.makedirs(transformed_dir, exist_ok=True)\n",
    "\n",
    "transformed_data_path = os.path.join(transformed_dir, \"transformed_data.csv\")\n",
    "merged_data.to_csv(transformed_data_path, index=False)\n",
    "\n",
    "print(\"\\nData transformation and enrichment completed. Transformed data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b1ffd-1f12-4ee5-b125-510cfb222c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2591f1-81bc-43b0-8eaa-486e2fe58dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
